# 初级阶段

## 一、概述

### 1.1是什么

> 大数据中使用较多的全文搜索的**搜索引擎！**对比 Solr (也是使用较多的搜索引擎)

### 1.2为什么使用ElasticSearch

> 之前的小型项目中的搜索功能，我们都是使用MySQL的select语句完成，在数据量较小的情况下，这种方式(MySQL)配合全文索引确实可用。但是在遇到大数据量的情况下，就会对MySQL造成很大的压力。

### 1.3使用的案例

> 目前浏览器上面网站使用的搜索功能基本上都是使用这一类专业的搜索引擎，功能强大。(例如：百度、京东、淘宝、github等)，使用过都知道在使用这些网站搜索的时候，使用体验好的多，所以专业的事情还是得专业的人来做。当然它也可以用作数据库，但是并不推荐。

### 1.4认识一个人(大佬)

推荐阅读文章：《Hadoop之父——Doug Cutting》https://www.cnblogs.com/doit8791/p/9556821.html

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590320967044-f9a4a883-32dd-4760-9526-e7c2df839e76.png)![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590321538032-f0de78e5-04e1-4fa4-b42c-5e2f4941d899.png)![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590321150817-3199acef-f5b5-4398-9181-957c2f4f2b8c.png)

这个人就是目前如日中天的大数据框架Hadoop的创始人：***Doug Cutting\***

不仅如此**Lucene(Java编写的全文搜索框架，****第一个提供全文文本搜索的开源函数库****)**也是出自于它之手,目前由Apache软件基金会就行管理。Hadoop就是Cutting在Lucene上利用Google的公开技术打造出来的。

而Solr和ElasticSearch就是以Lucene为核心进行封得到的。

我们的学习也是这三者对比分析进行学习。



### 1.5关于ELK

E：ElasticSearch(搜索、存储数据)

L：Logstash(收集，清洗数据)

K：Kibana(分析、展示数据)

> 这一套是大数据中处理日志数据分析必学的。

### 1.6对比Solr

| 对比项       | Solr                             | ElasticSearch                              |
| ------------ | -------------------------------- | ------------------------------------------ |
| 安装         | 较为复杂                         | 开箱即用                                   |
| 分布式管理   | 使用Zookeeper进行分布式管理      | 自带分布式协调管理功能                     |
| 数据格式支持 | json、xml、csv                   | 仅支持json                                 |
| 功能         | 功能丰富                         | 注重于核心(搜索)高级功能通过第三方插件实现 |
| 优势、劣势   | 查询已有数据时速度快更新索引时慢 | 建立索引快，查询较慢及时性查询快           |
| 应用场景     | 电商项目                         | Facebook、微博实时搜索。                   |



## 二、安装

### 2.1软件包

- ElasticSearch+Kibana：https://www.elastic.co/cn/start
- ElasticSearch-head：https://github.com/mobz/elasticsearch-head/releases

也可以使用Google插件：ElasticSearch Head - web前端展示插件

- ik分词器：https://github.com/medcl/elasticsearch-analysis-ik/releases

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590326617328-c601b12c-5db2-4212-8a26-fb09dedaf369.png)

### 2.2查看目录

ElasticSearch:

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590327915395-2d7b528a-af85-457f-aa3f-21e8138f952b.png)

```
1. bin: 启动文件目录
2. config: 相关配置文件目录
    - jvm.options 虚拟机相关的配置
    > 其中有个重要的配置：-Xms和-Xmx 都是1G,JVM中讲过这是项目运行占用的最小/最大内存空间，若内存不是很充裕可以调整到256M
  - log4j2。properties Log4j的配置文件
  - elasticsearch.yml ElasticSearch的核心配置文件
    > 里面有关于集群、路径、内存、网络等配置
    > 默认端口是 9200！！
3. jdk: 运行的java环境  
4. lib: 相关的库
   > 主要是ElasticSearch的套件以及Log4j、Lucene、jackson
5. logs: 日志输出文件夹
6. modules: 一些功能模块
7. plugins: 存放插件的目录
     > 当我们需要使用插件的时候只需要将插件的包放入即可(例如待会会使用的ik分词器)
```



### 2.3启动ElasticSearch

直接运行bin目录下的elasticsearch即可：显示默认端口9200:

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590329068209-03b7778d-2330-4608-9d44-4b9bcac5d7c1.png)

直接访问，可以获取如下的数据信息：

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590329153756-e41bfdd2-57c1-40cc-b302-f9742a556661.png)

### 2.4使用可视化工具（elasticsearch-head）

如果是使用Chrome插件那么这里可以跳过。重点讲使用node.js启动遇到的问题

在解压文件后，看到package.json就可以看出这是使用webpack构建的项目

1. 首先要进行资源依赖导入

由于我们之前配置了国内加速所以使用：`cnpm install`

1. 然后使用`npm run start`启动

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590330327037-227c959a-30e9-4040-8dd4-1613062b5c57.png)

1. 访问localhost:9100，进入head界面

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590330379398-c57512e5-e994-4554-934f-772f1258d49c.png)

这个界面和使用插件的一模一样的，但是问题就在于，使用插件是可以直接连接到我们已经启动的ElasticSearch的，但是使用这种方法就会产生跨域的问题。

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590330641589-04b02747-af2f-415a-8768-12338285a507.png)

在elasticsearch.yml中配置 ：

```yml
http: 
  cors: 
    enabled: true
    allow-origin: "*"
# 开启支持跨域，并使得所有人可以访问
```

然后保存再次启动

现在就可以成功连接了：

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590331060077-e201a174-6f4e-4287-9648-3269217e7b1a.png)

初学时，这里面索引就相当于一个数据库。存放数据的是一个文档。

这个网站我们学习过程中作为数据查看，具体查询存储操作使用Kibana进行。



### 2.5Kabana安装

一定要和ElasticSearch的版本一致！

解压耗时，这是一个标准的前端工程。
解压完成后直接启动/bin/kibana.bat就可以启动

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590332051218-81a2c31a-61b0-49f9-ab20-fd3f7bba8acf.png)

默认端口是5601，访问测试：

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590332113861-20c7a46f-5947-4c74-beba-685cc6be12ca.png)

一个异常炫酷舒服的界面！找到Dev Tools进入命令行界面，这就是我们日后经常使用的命令的位置：

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590332326508-0bcca572-f421-4945-975e-a01928e8693f.png)如果觉得英文界面不适应，也可以进行汉化！

在kabana的x-pack目录下的插件目录中有一个translations目录。其中就放着国际化的两个配置文件其中就有zh-CN.json

```
kibana-7.7.0-windows-x86_64\x-pack\plugins\translations\translations\zh-CN.json
```

然后我们在kabana的config目录中的kabana.yml中配置：

```
i18n.locale: "zh-CN"
```

重新启动即可。

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590332790587-6ea3a120-a6a6-495c-a2c7-7f65520c0ef8.png)



## 三、ElasticSearch使用

### 3.1核心概念



与关系型数据库对比：

| **RelationDB**   | **ElasticSearch** |
| ---------------- | ----------------- |
| 数据库(DataBase) | 索引(indices)     |
| 表(Table)        | types(即将过时)   |
| 行(row)          | 文档(documents)   |
| 字段(columns)    | field             |

-  ElasticSearch所有的数据都是json
- 单个ElasticSearch也是一个集群，集群的默认名字就是ElasticSearch

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590334480856-7d3d4896-e3fe-454d-8b66-10dafdf4f1a0.png)

> 文档

ElasticSearch是面向文档的，一个索引中可以有多个文档，搜索的最小单位就是文档，以key:value形式设置。



> 类型

也就是文档中value的数据类型，与关系型数据库一样，相当于一个二级目录

数据都需要有指定类型，但是ElasticSearch可以自行推断但是难免有错误的时候。



> 索引

索引相当于一个数据库，是多个文档的集合

每个索引中都包含五个分片，这五个分片底层就是5个Lucene索引，索引的分片相当于MySQL的分库。

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590335447945-c7ad8368-73f2-4756-887e-27b5af85d329.png)

Lucene索引中用到了**倒排索引**

> **倒排索引：**实现“单词-文档矩阵”的一种具体存储形式，通过倒排索引，可以根据单词快速获取包含这个单词的文档列表。倒排索引主要由两个部分组成：“单词词典”和“倒排文件”。
>
> 
>
> 参考阅读:https://blog.csdn.net/starzhou/article/details/87519973



### 3.2 IK分词器

什么是分词器？

> 将一个中文单词或者句子拆分成多个关键字，然后搜索的时候直接对这些关键字进行匹配筛选。
>
> 但是有时候过度的拆分反而会带来负面效果，所以使用ik分词器解决这个问题。

如果大量使用的是中文搜索，建议使用ik分词器



IK分词器提供了两个分词算法：

- **`ik_smart`** ：最少切分
- **`ik_max_word`** : 最细粒度拆分



将ik分词器的解压文件放入ElasticSearch中的plugins目录下，然后重启ElasticSearch。

可以看到加载了ik分词器插件：

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590336690720-e4aef931-1924-4471-a592-ae0f77656f8e.png)

或者使用`elasticsearch-plugin list`命令也可以查看使用了的插件。

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590336661289-2adda762-d6b4-48a2-af39-ef32028aac00.png)



### 3.3测试使用两种分词算法

```json
**ik_smart**
GET _analyze
{
  "analyzer": "ik_smart",// 使用最少划分算法
  "text": ["中国共产党"]
}
```

结果：

```json
{
  "tokens" : [
    {
      "token" : "中国共产党",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "CN_WORD",
      "position" : 0
    }
  ]
}
```



**`ik_max_word`**

```json
GET _analyze
{
  "analyzer": "ik_max_word",// 使用最小粒度算法
  "text": ["中国共产党"]
}
```

结果：

```json
{
  "tokens" : [
    {
      "token" : "中国共产党",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "CN_WORD",
      "position" : 0
    },
    {
      "token" : "中国",
      "start_offset" : 0,
      "end_offset" : 2,
      "type" : "CN_WORD",
      "position" : 1
    },
    {
      "token" : "国共",
      "start_offset" : 1,
      "end_offset" : 3,
      "type" : "CN_WORD",
      "position" : 2
    },
    {
      "token" : "共产党",
      "start_offset" : 2,
      "end_offset" : 5,
      "type" : "CN_WORD",
      "position" : 3
    },
    {
      "token" : "共产",
      "start_offset" : 2,
      "end_offset" : 4,
      "type" : "CN_WORD",
      "position" : 4
    },
    {
      "token" : "党",
      "start_offset" : 4,
      "end_offset" : 5,
      "type" : "CN_CHAR",
      "position" : 5
    }
  ]
}
```



后者将所有的可能的单词拆分出来。

但是有可能将单词过度拆分，反而达不到想要的效果：

```json
GET _analyze
{
  "analyzer": "ik_max_word",
  "text": ["我是官宇辰"]
}
```

结果：

```json
{
  "tokens" : [
    {
      "token" : "我",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "CN_CHAR",
      "position" : 0
    },
    {
      "token" : "是",
      "start_offset" : 1,
      "end_offset" : 2,
      "type" : "CN_CHAR",
      "position" : 1
    },
    {
      "token" : "官",
      "start_offset" : 2,
      "end_offset" : 3,
      "type" : "CN_CHAR",
      "position" : 2
    },
    {
      "token" : "宇",
      "start_offset" : 3,
      "end_offset" : 4,
      "type" : "CN_CHAR",
      "position" : 3
    },
    {
      "token" : "辰",
      "start_offset" : 4,
      "end_offset" : 5,
      "type" : "CN_CHAR",
      "position" : 4
    }
  ]
}
```

官宇辰是一个名字，但是被过度拆分成了三个独立的关键字，降低了搜索的精确性。这种情况我们需要将这个单词配置到分词器字典中，让ElasticSearch知道这是一个完整的单词。



再ElasticSearch的plugins中找到ik分词器的目录并进入config：

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590384539634-77d982ca-e208-4166-9884-d1e4832237c6.png)

所有`.dic`后缀的都是一个个字典，可以直接打开查看：

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590384607475-c2f68269-44e5-4d69-a84f-99c8ea009459.png)仅仅这些自带的字典是无法满足于我们的需求的，我们可以创建一个自己的字典例如：sakura.dic 打开字典然后加入我们自定义的单词`官宇辰`

然后打开IKAnalyzer.cfg.xml把我们自己的字典添加进去：

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590384969910-6bc1ffe1-4ad6-4430-8d86-4dafd61ec05b.png)

最后重启ElasticSearch和Kibana，启动日志中也会显示加载自定义的词典：

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590385132733-f2d1d702-e3b4-4d63-860e-0ea9a0629129.png)

再次测试：

```json
{
  "tokens" : [
    {
      "token" : "我",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "CN_CHAR",
      "position" : 0
    },
    {
      "token" : "是",
      "start_offset" : 1,
      "end_offset" : 2,
      "type" : "CN_CHAR",
      "position" : 1
    },
    {
      "token" : "官宇辰",
      "start_offset" : 2,
      "end_offset" : 5,
      "type" : "CN_WORD",
      "position" : 2
    }
  ]
}
```

效果大不相同，这次`官宇辰`以完整的单词出现。



### 3.4结合RestFul风格使用

| **method** | **url**                                     | **功能描述**     |
| ---------- | ------------------------------------------- | ---------------- |
| PUT        | localhost:9200/索引名/类型名/文档id         | 创建文档并指定id |
| POST       | localhost:9200/索引名/类型名                | 创建文档随机id   |
| POST       | localhost:9200/索引名/类型名/文档id/_update | 修改指定id的文档 |
| DELETE     | localhost:9200/索引名/类型名/文档id         | 删除指定id的文档 |
| GET        | localhost:9200/索引名/类型名/文档id         | 查询指定id的文档 |
| POST       | localhost:9200/索引名/类型名/_search        | 查询所有数据     |

#### 创建索引并新建文档：

```json
PUT /test01/_doc/1
{
  "name": "sakura",
  "age": 20
}
```

注意点：

1. 在Kabana中测试localhost:9200前缀是不用写的！
2. 类型名(type)在一个索引中只能有一个type
3. type在今后的版本中可能被启用，url中的类型名使用`_doc`隐式代替



结果：

```json
{
  "_index" : "test01",
  "_type" : "_doc",
  "_id" : "1",
  "_version" : 1,
  "result" : "created",
  "_shards" : {
    "total" : 2,
    "successful" : 1,
    "failed" : 0
  },
  "_seq_no" : 0,
  "_primary_term" : 1
}
```

信息：

- (_index)操作的索引是：test01
- (_type)类型是：_doc
- (_id)文档的id是：1
- (result)操作类型是：created(创建)
- (_shards)分片的信息



在上面的测试中我们的字段没有设置类型，我们来查询一下：



#### 查询文档

```
GET /test01/_doc/1
```

结果：

```json
{
  "_index" : "test01",
  "_type" : "_doc",
  "_id" : "1",
  "_version" : 1,
  "_seq_no" : 0,
  "_primary_term" : 1,
  "found" : true,
  "_source" : {
    "name" : "sakura",
    "age" : 20
  }
}
```

在_source中可以看到文档的字段的值

如果要查看索引的信息(包括字段的类型信息)：

```
GET /test01
```

结果：

```json
{
  "test01" : {
    "aliases" : { },
    "mappings" : {
      "properties" : {
        "age" : {
          "type" : "long"
        },
        "name" : {
          "type" : "text",
          "fields" : {
            "keyword" : {
              "type" : "keyword",
              "ignore_above" : 256
            }
          }
        }
      }
    },
    "settings" : {
      "index" : {
        "creation_date" : "1590389970005",
        "number_of_shards" : "1",
        "number_of_replicas" : "1",
        "uuid" : "jdkvBIGwSw6jOPLdvwDsOw",
        "version" : {
          "created" : "7070099"
        },
        "provided_name" : "test01"
      }
    }
  }
}
```

在mapping中的properties中就可以看到字段的详细信息，ElasticSearch根据数据类型自动推断数据类型，并且text类型的数据会默认底层设置为关键字(也即不可拆分单词)。

其次setting中给出的是当前索引的信息。



#### 带类型创建索引

先来了解有哪些数据类型：

| **一级分类** | **二级分类** | **具体类型**                         |
| ------------ | ------------ | ------------------------------------ |
| 核心类型     | 字符串类型   | string,text,keyword                  |
| h            | 整数类型     | integer,long,short,byte              |
| h            | 浮点类型     | double,float,half_float,scaled_float |
| h            | 逻辑类型     | boolean                              |
| h            | 日期类型     | date                                 |
| h            | 范围类型     | range                                |
| h            | 二进制类型   | binary                               |
| 复合类型     | 数组类型     | array                                |
| f            | 对象类型     | object                               |
| f            | 嵌套类型     | nested                               |
| 地理类型     | 地理坐标类型 | geo_point                            |
| d            | 地理地图     | geo_shape                            |
| 特殊类型     | IP类型       | ip                                   |
| t            | 范围类型     | completion                           |
| t            | 令牌计数类型 | token_count                          |
| t            | 附件类型     | attachment                           |
| t            | 抽取类型     | percolator                           |

```json
PUT /test02
{
  "mappings": {
    "properties": {
      "name":{
        "type": "text"
      },
      "age":{
        "type": "integer"
      },
      "birth":{
        "type": "date"
      }
    }
  }
}
```

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590391708959-1c8909c6-96a3-4245-b9b4-a633d1182e99.png)此时索引中还没有文档，我们看看索引的信息

```json
{
  "test02" : {
    "aliases" : { },
    "mappings" : {
      "properties" : {
        "age" : {
          "type" : "integer"
        },
        "birth" : {
          "type" : "date"
        },
        "name" : {
          "type" : "text"
        }
      }
    },
    "settings" : {
      "index" : {
        "creation_date" : "1590391599265",
        "number_of_shards" : "1",
        "number_of_replicas" : "1",
        "uuid" : "oACx62V4TYaCO_RDeHI6Cg",
        "version" : {
          "created" : "7070099"
        },
        "provided_name" : "test02"
      }
    }
  }
}
```

指定了类型之后相当于建立了一个规则，后面插入进来的文档都要按着这个规则设置数据！



#### 插入数据

正常按字段类型插入：

```json
PUT /test02/_doc/1
{
  "name" : "官宇辰",
  "age" : 20,
  "birth" : "2000-08-17"
}
```

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590392007690-860834fe-cc19-4499-a137-49d1ea6bdb65.png)数据解析正常！

插入数据类型于字段类型不符：

```json
PUT /test02/_doc/2
{
  "name" : 1234567,
  "age" : "sakura",
  "birth" : "2000-07-18"
}
```

这样会直接报错400-BadRequest！但是 字符串和数字是可以自动转化的：

```json
PUT /test02/_doc/2
{
  "name" : 1234567,
  "age" : "20",
  "birth" : "2000-07-18"
}
```

这样就不会报错，但是查询出来的数据还是字符串/数字

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590392419822-ebc164a1-75f7-4edb-974c-eb147f37d247.png)



#### 扩展：查看ElasticSearch的相关信息

```
GET _cat/xx

_cat/indices 查看所有索引信息
_cat/plugins 查看使用的插件
_cat/nodes 查看节点信息
```



#### 修改文档

方式一：直接PUT进行数据覆盖

```json
PUT /test02/_doc/2
{
  "name" : "sakura",
  "age" : 19,
  "birth" : "2000-07-18"
}
```

每次修改 version都会自增1。

缺点：一旦漏掉了其中一个属性，那么原有的属性会被空值覆盖。



方式二：使用xx/_update

```json
POST /test02/_doc/2/_update
{
  "doc":{
    "name":"sakura",
    "birth" : "2000-08-17"
  }
}
```

指定字段进行修改，但是结果显示建议我们修改请求的url：

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590394052252-e5abe878-1a51-415d-b829-987dcbcf28db.png)

\#!弃用: [类型删除] 更新请求中指定文档的类型已弃用，请改用端点/{index}/_ update/{id}。

所以我们的正确的url应该是：

```json
POST /test02/_update/2
{
  "doc":{
    "name":"sakura",
    "birth" : "2000-08-17"
  }
}
```



#### 删除索引/文档

删除文档

```json
DELETE /test02/_doc/2
```

删除索引

```json
DELETE /test02
```



### 3.5复杂查询



#### 基础功能

刚开始我们只使用了最基本的查询方式 `GET xx/x` 现在来了解一下使用复杂查询

1. 查询索引中所有文档数据

```json
POST /test01/_search
```

随着新版更新，类型被逐渐弃用。

```json
{
  "took" : 0,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 3,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "test01",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 1.0,
        "_source" : {
          "name" : "sakura",
          "age" : 20
        }
      },
      {
        "_index" : "test01",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 1.0,
        "_source" : {
          "name" : "老王",
          "age" : 35
        }
      },
      {
        "_index" : "test01",
        "_type" : "_doc",
        "_id" : "3",
        "_score" : 1.0,
        "_source" : {
          "name" : "官宇辰ood",
          "age" : 20
        }
      }
    ]
  }
}
```

完整版：

```json
GET /test01/_search
{
  "query": {
    "match_all": {}
  }
}
```



2. 按条件查询

```json
POST /test01/_search?q=name:sakura
{
  "took" : 0,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 1,
      "relation" : "eq"
    },
    "max_score" : 1.2800651,
    "hits" : [
      {
        "_index" : "test01",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 1.2800651,
        "_source" : {
          "name" : "sakura",
          "age" : 20
        }
      }
    ]
  }
}
```



3. 按条件查询(二)

```json
GET /test01/_search
{
  "query": {
    "match": {
      "name": "sakura"
    }
  }
}
```

> 这种查询方式中 match不支持多字段查询！



4. 查询显示部分字段

使用`_source`指定要查询显示的字段

```json
GET /test01/_search
{
  "query": {
    "match": {
      "name": "sakura"
    }
  },
  "_source": ["name"]
}
```

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590416799499-ca724d90-c201-411e-b798-9542af93df8e.png)



5. 查询结果排序

使用`sort`来指定用于排序的字段

```json
GET /test01/_search
{
  "query": {
    "match": {
      "name": "sakura"
    }
  }, 
  "sort": [
    {
      "age": {
        "order": "asc"
      }
    },
    {
      "_score": {
        "order": "desc"
      }
    }
  ]
}
```

- 可以设置多个排序的字段
- 使用排序后max_score会变为null。



6. 分页查询

```json
GET /test01/_search
{
  "query": {
      "match": {
        "name": "sakura"
      }
  },
  "from": 0,
  "size": 2
}
```

from、size两个参数的值对应SQL中limit的两个值。



#### 关于score和hits

在我们的查询结果中，所有的查询记录使用一个hits对象呈现：

```json
"hits" : {
    "total" : {
      "value" : 2,
      "relation" : "eq"
    },
    "max_score" : 0.6133945,
    "hits" : [
      {
        "_index" : "test01",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 0.6133945,
        "_source" : {
          "name" : "sakura"
        }
      },
      {
        "_index" : "test01",
        "_type" : "_doc",
        "_id" : "3",
        "_score" : 0.36372143,
        "_source" : {
          "name" : "官宇辰sakura"
        }
      }
    ]
  }
```

`total`：对查询结果的一个统计。

`max_score`：表示结果查询结果中最高匹配度。

`hits`：一个查询结果的数组，其中包含了一个个查询结果对象。

每一个hit中都有一个`_score`这个数值代表着这条记录与查询条件的匹配程度，分值越高匹配度越高。



#### 多条件查询(must)

```json
GET /test01/_search
{
  "query": {
    "bool": {
      "must": [
        {
          "match": {
            "name": "sakura"
          }
        },
        {
          "match": {
            "age": "20"
          }
        }
      ]
    }
  }
}
```

首先我们要用到 query里面的bool查询，然后使用must

must是一个条件数组，可以同时放多个条件，查询结果必须同时满足这些条件！

#### 多条件查询(should)

与must功能类似，但是条件数组中只要满足一个就会加入结果集中。

#### 多条件查询(not must)

与must功能刚好相反，条件数组中的条件必须同时不满足。

#### 多值查询

在查询时候我们会碰到这种情况：

要求结果的值是在给定的可选值之中的：例如MySQL中的in

```json
GET /test01/_search
{
  "query": {
      "match": {
        "name": "sakura 官宇辰"
      }
  }
}
```

比如例子中就是搜 name in {"sarkua","官宇辰"}



#### 过滤查询(range)

```json
GET /test01/_search
{
  "query": {
    "bool": {
      "should": [
        {
          "match": {
            "name": "sakura"
          }
        }
      ],
      "filter": [
        {
          "range": {
            "age": {
              "gte": 10,
              "lte": 20
            }
          }
        }
      ]
    }
  }
}
```

也是bool查询中的，filter中主要是对查询的结果进行一写过滤操作：

常用有

- `range`判断一个数值的在某个区间内。

- - gt：大于
  - gte：大于等于
  - lt：小于
  - lte：小于等于

- `exist`确认查询结果中某个字段有值而不是空



#### 精确查找(term)

与match的区别：

- term是精确查询：基于倒排索引，不会对输入词进行分词解析

> 必须在内容中能够找到完整的词才能作为结果

- match则是模糊查询，会首先对输入词进行分词，然后将匹配的文档作为结果

**实例：**

这个是我们当前的数据：

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590480508009-c48a57fc-8e89-4267-a66c-98ae7f07a998.png)

测试对比一下使用match、term分别搜索sakura：

```json
GET /test01/_search
{
  "query": {
      "match": {
        "name": "sakura"
      }
  }
}

GET /test01/_search
{
  "query": {
      "term": {
        "name": {
          "value": "sakura"
        }
      }
  }
}
```

最终两者得出的结果是一样的，这是因为英文单词或者英文和数字组合的字符串在存储的时候都不会被分词！也就是说数据中的`sakura、官宇辰sakura`在存储过程中都保留了sakura这个完整的词！



那么我们再来测试一下搜"官宇辰"呢？

结果是：

- match正常，搜到了两条，并且后来我测试就算是name是"官宇"、"宇辰"等只要之中有这个三个字的文档都会被检索到！
- 而term则是一条都没有查到

现象解释：

> 这个现象很好地解释了之前的对比
>
> - match搜索时候，将搜索词"官宇辰"，拆分成了三个单独的字！然后去匹配文档，而又由于文档的name字段是text类型在存储的时候使用默认分词，中文词被拆分成多个字，所以都能匹配。
> - 反观term，在搜索的时候搜索词"官宇辰"是没有被处理的，需要文档中有完整的"官宇辰"这个词才行，而文档中都是一个个汉字，所以无一匹配。

!!!!match在对搜索词进行分词的时候，是参照字段类型进行分词的!!!当字段是keyword属性时，使用match也不会对搜索词分词。



使用terms也可以实现相同与match的效果：

```json
GET /test01/_search
{
  "query": {
      "terms": {
        "name": [
          "官",
          "宇",
          "辰"
        ]
      }
  }
}
```

使用terms，只需要保证所给的字段能匹配所给的词列表中任意一个就可以。



#### keyword和text

两个类型的区别关键在于数据存储/搜索的时候是否会被进行分词处理。

```json
POST _analyze
{
  "analyzer": "keyword", 
  "text": ["sakura官宇辰gyc"]
}

POST _analyze
{
  "analyzer": "default", 
  "text": ["sakura官宇辰gyc"]
}
```

结果：

keyword：

```json
{
  "tokens" : [
    {
      "token" : "sakura官宇辰gyc",
      "start_offset" : 0,
      "end_offset" : 12,
      "type" : "word",
      "position" : 0
    }
  ]
}
```

text：

```json
{
  "tokens" : [
    {
      "token" : "sakura",
      "start_offset" : 0,
      "end_offset" : 6,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "官",
      "start_offset" : 6,
      "end_offset" : 7,
      "type" : "<IDEOGRAPHIC>",
      "position" : 1
    },
    {
      "token" : "宇",
      "start_offset" : 7,
      "end_offset" : 8,
      "type" : "<IDEOGRAPHIC>",
      "position" : 2
    },
    {
      "token" : "辰",
      "start_offset" : 8,
      "end_offset" : 9,
      "type" : "<IDEOGRAPHIC>",
      "position" : 3
    },
    {
      "token" : "gyc",
      "start_offset" : 9,
      "end_offset" : 12,
      "type" : "<ALPHANUM>",
      "position" : 4
    }
  ]
}
```



#### 高亮查询

在许多搜索界面，搜索到的词都会以高亮的形式显示出来，ElasticSearch支持搜索结果高亮

```json
GET /test01/_search
{
  "query": {
    "match": {
      "name": "官宇辰"
    }
  },
  "highlight": {
    "fields": {
      "name": {}
    }
  }
}
```

在highlight的field中设置要显示高亮的字段，在搜索结果中该字段中匹配的词就会出现高亮。

部分结果：

```json
    "hits" : [
      {
        "_index" : "test01",
        "_type" : "_doc",
        "_id" : "3",
        "_score" : 1.8696017,
        "_source" : {
          "name" : "官宇辰sakura",
          "age" : 20
        },
        "highlight" : {
          "name" : [
            "<em>官</em><em>宇</em><em>辰</em>sakura"
          ]
        }
      },
      {
        "_index" : "test01",
        "_type" : "_doc",
        "_id" : "5",
        "_score" : 1.4723402,
        "_source" : {
          "name" : "官宇",
          "age" : 22
        },
        "highlight" : {
          "name" : [
            "<em>官</em><em>宇</em>"
          ]
        }
      }
    ]
```

默认是使用<em>标签进行包裹。

来看看百度中使用的效果：

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1461246/1590483717025-1a3bc61a-2230-4e69-b947-e0e68d8fc7d5.png)

当然这个标签是可以修改的：

```json
"highlight": {
    "pre_tags": "<span class='key' style='color: red'}>",
    "post_tags": "</span>", 
    "fields": {
      "name": {}
    }
}
```

设置一下pre_tags和post_tags就可以定制高亮标签了！

## 四、SpringBoot集成ES

### 项目搭建准备

首先找去官网找到帮助文档：

<img src="https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200526203015.png" alt="image-20200526203015162" style="zoom:60%;" /><img src="https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200526203053.png" alt="image-20200526203052955" style="zoom:50%;" />

选择JavaRESTClient，然后选择High Level Rest Client；注意一定选择和自己使用相同版本的文档。



然后使用SpringBoot构建项目的时候，☑️NoSQL -> Spring Data ElasticSearch！

![image-20200526202601147](https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200526202601.png)

ElasticSearch相关的依赖：

<img src="https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200526221218.png" alt="image-20200526221218193" style="zoom:67%;" />

**SpringBoot2.6.0版本中使用的ElasticSearch-rest-high-level-client版本是6.8.7**。。我们必须将其切换到和我们使用的ES保证同一个版本才能稳定连接。

在SpringBoot的pom.xml文件中使用一个变量来确定ES的版本，我们在自己的项目中配置这个变量即可：

![image-20200526221851296](https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200526221851.png)

修改项目的pom.xml

![image-20200526222150775](https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200526222150.png)

然后刷依赖，等待下载完毕：

![image-20200526222250403](https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200526222250.png)



### 初始化准备

1. 官方文档中说明需要一个`RestHighLevelClient`实例，来完成请求：

   ![image-20200526224005250](https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200526224005.png)

   我们可以创建一个ElasticSearch的配置类，然后以Bean的形式创建这样一个实例。

2. 编写配置类和Bean

   ```java
   @Configuration
   public class ElasticSearchClientConfig {
   
       @Bean
       public RestHighLevelClient restHighLevelClient() {
           RestHighLevelClient restHighLevelClient =
                   new RestHighLevelClient(
                           RestClient.builder(
                                   new HttpHost("localhost", 9200, "http")
                           ));
           return restHighLevelClient;
       }
   }
   ```

   ![image-20200526224800476](https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200526224800.png)

   在配置Bean完成之后，我们来简单看看ElasticSearch的源码其实原本是有一个RestHighLevelClient的默认实例，点过去ElasticSearch的源码大陆。

   

3. 源码浏览

   - 来到spring.factories文件中找到ElasticSearch相关的包以及类：

     ![image-20200526225854279](https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200526225854.png)

     一共11项还是很好找到的，也就两个包 data和elasticsearch，Let’s Go。

   - 锁定包的位置以及大致内容：

     data包：(这个包里面放了几乎所有数据库相关的自动配置相关的类，jdbc、solr、redis等)

     <img src="https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200526230220.png" alt="image-20200526230220362" style="zoom:67%;" />

     elasticsearch包：(由于我们使用的的RestClient，所以重点在rest包中)

     <img src="https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200526230409.png" alt="image-20200526230409893" style="zoom:67%;" />

   - 我们从最上面看起

     - ElasticSearchAutoConfigration：

       <img src="https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200526230654.png" alt="image-20200526230654648" style="zoom: 67%;" />

       仅有一个TransportClient的Bean，这个类相对与我们使用的RestClient，其使用的api更加接近与原生操作。

     - 对应的ElasticSearchProperties

       其中的内容也比较少，大多是关于集群的配置：

       <img src="https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200526231116.png" alt="image-20200526231116024" style="zoom: 67%;" />

       <img src="https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200526231200.png" alt="image-20200526231200416" style="zoom: 80%;" />

     - ElasticsearchDataAutoConfiguration中主要是导入了ElasticsearchDataConfiguration中的东西

       ![image-20200526231656037](https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200526231723.png)

       我们直接看后者：

       一看才发现导入的都是静态内部类：

       ![image-20200526231857330](https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200526231857.png)

       其中的`RestClientConfiguration`类关注一下，使用RestHighLevelClient创建了一个ElasticsearchRestTemplate(见到Template瞬间严肃，这个可能就是我们调用api需要的类了)

       ![image-20200526232142309](https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200526232142.png)

       果不其然，这个类中的方法可以够测了：截取部分：

       <img src="https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200526234730.png" alt="image-20200526234730230" style="zoom:80%;" />

       

       后面六个就不看了，继续看elasticsearch.rest包里面的类把。

     - 首先：RestClientAutoConfiguration，和上面如出一辙导入一堆类，没猜错应该也是静态内部类

       ![image-20200526235119539](https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200526235119.png)

     - 看看RestClientConfigurations类

       不出所料果然是三个静态内部类：

       ![image-20200526235308792](https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200526235308.png)

       中间的RestHighLevelClientConfiguration引起注意！！

       <img src="https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200527000929.png" alt="image-20200527000929035" style="zoom:67%;" />

       默认情况下这个类提供了两个Bean：RestHighLevelClient和RestClient，但是我们配置了我们自己的RestHighLevelClient，所以这个Bean也就失效了。

     - 然后就是RestClientProperties，里面是一些关于客户端连接的配置，

       默认连接uri是：http://localhost:9200

       连接超时是1s，读取超时是30s。

       ![image-20200527001412356](https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200527001412.png)

     - 最后最后是一个接口RestClientBuilderCustomizer，用于定制化RestClient，咱估计也用不到，直接略过了。

   - OK，以上就是一次简单的源码浏览，(不敢细读😄)



### 基本API使用

在API调用部分学习之前，要理清一个概念：我们之前使用的命令行操作，其实都是一个个带请求的请求，对应的结果，则是ElasticSearch对于请求反馈的响应。所以在使用Java进行调用时候，明确我们==发出的是请求==！所以==一切操作都需要创建一个请求然后给客户端去处理和响应。==

#### 索引操作：

- 创建索引

  ```java
  @Autowired
  @Qualifier("restHighLevelClient")
  private RestHighLevelClient client;
  
  @Test
  void testCreateIndex() throws IOException {
     // new一个创建索引的请求,索引名为test_index01
     CreateIndexRequest request = new CreateIndexRequest("test_index01");
     // 客户端发送请求
     CreateIndexResponse response = client.indices().create(request, RequestOptions.DEFAULT);
  }
  ```

  ![image-20200527142805858](https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200527142806.png)

  目前这个索引什么都没有，稍后在文档的API操作中使用

- 查询索引是否存在

  ```java
  @Test
  void testQueryIndex() throws IOException {
     // new 一个判断索引是否存在的请求
     GetIndexRequest request = new GetIndexRequest("test_index01");
     // 客户端发送请求
     boolean isExists = client.indices().exists(request, RequestOptions.DEFAULT);
  
     System.out.println(isExists);
  }
  ```

  基本上GetIndexRequest可以用于请求关于索引的信息。

- 删除索引

  ```java
  @Test
  void testDeleteIndex() throws IOException {
     // new 一个删除索引的请求
     DeleteIndexRequest request = new DeleteIndexRequest("test_index01");
     // 客户端发送请求
     AcknowledgedResponse response = client.indices().delete(request, RequestOptions.DEFAULT);
  }
  ```



#### 文档操作

- 创建文档

  ```java
  @Test
  void testAddDocument() throws IOException {
     User user = new User("sakura", 19);
     // 创建一个请求
     IndexRequest request = new IndexRequest("test_index01");
     // put /test_index/_doc/1 {...}
     request.id("1");
     // 设置超时时间
     request.timeout(TimeValue.timeValueSeconds(1));
     // 放入请求体
     request.source(JSON.toJSONString(user), XContentType.JSON);
  
     IndexResponse response = client.index(request, RequestOptions.DEFAULT);
     System.out.println(response.status());
     System.out.println(response.toString());
  }
  ```
  
  在命令行操作中我们创建文档的都需要一个请求体，存放我们要插入的数据。
  在调用API时候，将实体类转化为json字符串然后附加给请求就可以达到相同的效果。
  
  这是响应信息：
  
  ```txt
  CREATED
  IndexResponse[
      index=test_index01,
      type=_doc,
      id=1,
      version=1,
      result=created,
      seqNo=0,
      primaryTerm=1,
      shards=
      {
       "total":2,
       "successful":1,
       "failed":0
      }
  ]
  ```
  
- 查询文档是否存在

  ```java
  @Test
  void testDocumentExist() throws IOException {
     GetRequest request = new GetRequest("test_index01", "2");
     boolean exists = client.exists(request, RequestOptions.DEFAULT);
     System.out.println(exists);
  }
  ```

  显然结果是False。

- 查看文档

  ```java
  @Test
  void testGetDocument() throws IOException {
     // 方式一
     GetSourceRequest request = new GetSourceRequest("test_index01", "1");
     GetSourceResponse response = client.getSource(request, RequestOptions.DEFAULT);
     System.out.println(response.getSource().toString());
  
     // 方式二
     GetRequest request1 = new GetRequest("test_index01", "1");
     GetResponse response1 = client.get(request1, RequestOptions.DEFAULT);
     System.out.println(response1.getSourceAsString());
  }
  ```

  两种方法都能够获取文档的source(字段信息)，前者只能以Map形式展示，后者功能更强大可以以json字符串形式展示；

  ![image-20200527164446213](https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200527164446.png)

  且第二种方式的响应还可以查看其他信息,第一种方式仅仅查看字段信息。

  ![image-20200527164541625](https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200527164541.png)

- 更新文档

  ```java
  @Test
  void testUpdateDocument() throws IOException {
     UpdateRequest request = new UpdateRequest("test_index01", "1");
     HashMap<String, Object> map = new HashMap<>();
     map.put("name", "官宇辰");
     map.put("age", 19);
     request.doc(map, XContentType.JSON);
     UpdateResponse response = client.update(request, RequestOptions.DEFAULT);
     System.out.println(response.status());
  }
  
  @Test
  void testUpdateDocument2() throws IOException {
     UpdateRequest request = new UpdateRequest("test_index01", "1");
     User user = new User("官宇辰", 20);
     request.doc(JSON.toJSONString(user), XContentType.JSON);
     UpdateResponse response = client.update(request, RequestOptions.DEFAULT);
     System.out.println(response.status());
  }
  ```

  也是两种方式：一种是map封装传参，一种的对象转json字符床传参。
  在使用命令行的时候，就说明过update请求中有一个doc对象，里面放着要修改的字段和值。

  ==使用map封装参数的的话可以完成部分字段的修改==，但是后者则是将一个新对象覆盖到旧记录上，有时候即使字段没有设置值也会使用默认值进行替换，比如int类型的字段默认就是0。

- 删除文档

  ```java
  @Test
  void testDeleteDocument() throws IOException {
     DeleteRequest request = new DeleteRequest("test_index01", "1");
     DeleteResponse response = client.delete(request, RequestOptions.DEFAULT);
     System.out.println(response.status());
  }
  ```

- 批量处理(bulk)

  ```java
  @Test
  void testBulkRequest() throws IOException {
     BulkRequest request = new BulkRequest("test_index01");
  
     List<User> users = new ArrayList<>();
     users.add(new User("AAAA", 19));
     users.add(new User("BBBB", 21));
     users.add(new User("CCCC", 18));
     users.add(new User("DDDD", 23));
     users.add(new User("EEEE", 16));
     users.add(new User("FFFF", 25));
     users.add(new User("GGGG", 22));
  
     for (int i = 0; i < users.size(); i++) {
        IndexRequest indexRequest = new IndexRequest("test_index01");
        indexRequest.id(String.valueOf(i));
        indexRequest.source(JSON.toJSONString(users.get(i)), XContentType.JSON);
        request.add(indexRequest);
     }
  
     BulkResponse response = client.bulk(request, RequestOptions.DEFAULT);
     // 是否有失败的请求
     System.out.println(response.hasFailures());
     System.out.println(response.status());
  
  }
  ```

  批处理是对多个请求进行集中处理。

  

- 搜索文档(基本操作)

  首先要确定请求的类型是`SearchRequest`

  需要使用`QueryBuilder`类及其工具类`QueryBuilders`，以及一个`SearchSourceBuilder`

  推荐阅读：<https://www.cnblogs.com/wenbronk/p/6432990.html>

  > **全查询match_all**

  ```java
  @Test
  void testSearch() throws IOException {
     SearchRequest request = new SearchRequest("test_index01");
  
     // 创建一个查询
     QueryBuilder queryBuilder = QueryBuilders.matchAllQuery();
     // 创建一个搜索
     SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();
  
     sourceBuilder.query(queryBuilder);
     request.source(sourceBuilder);
     SearchResponse response = client.search(request, RequestOptions.DEFAULT);
     SearchHits hits = response.getHits();
     Iterator<SearchHit> iterator = hits.iterator();
     while (iterator.hasNext()) {
        System.out.println(iterator.next());
     }
  }
  ```

  首先要确定查询条件：QueryBuilder，利用QueryBuilders工具类基本上可以搞定。

  然后把QueryBuilder给SearchSourceBuilder.source()，创建一个按查询条件搜索source的搜索

  然后再把这个SearchSourceBuilder设置到请求中，然后由客户端发起请求。

  > 结果的获取和输出：
  >
  > 我们在使用命令行就说过每一条记录都是用一个hit对象包裹起来的，在Java中这个对象是`SearchHit`
  >
  > 从响应中获取hits数组(`SearchHits`)，然后用迭代器输出或者遍历输出。

  结果(部分)一目了然：

  ```json
  {
    "_index" : "test_index01",
    "_type" : "_doc",
    "_id" : "0",
    "_score" : 1.0,
    "_source" : {
      "age" : 19,
      "name" : "AAAA"
    }
  }
  {
    "_index" : "test_index01",
    "_type" : "_doc",
    "_id" : "1",
    "_score" : 1.0,
    "_source" : {
      "age" : 21,
      "name" : "BBBB"
    }
  }
  
  ...
  ```

   

  > match条件查询

  ```java
  @Test
  void testMatch() throws IOException {
     SearchRequest request = new SearchRequest("test_index01");
     SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();
     QueryBuilder queryBuilder = QueryBuilders.matchQuery("name", "BBBB");
     sourceBuilder.query(queryBuilder);
     sourceBuilder.fetchSource("name", null);
  
     request.source(sourceBuilder);
     SearchResponse response = client.search(request, RequestOptions.DEFAULT);
     SearchHits hits = response.getHits();
     for (SearchHit hit : hits) {
        System.out.println(hit);
     }
  }
  ```

  中间用到了SearchSourceBuilder的`fetchSource`的方法,相当于我们请求体中`_source`的作用，即显示/排除哪些字段。
  结果：(果然只显示了name)

  ```json
  {
    "_index" : "test_index01",
    "_type" : "_doc",
    "_id" : "1",
    "_score" : 1.6739764,
    "_source" : {
      "name" : "BBBB"
    }
  }
  
  ```

  

   

  > 查询结果分页、排序

  ```java
  @Test
  void testSortAndPages() throws IOException {
     SearchRequest request = new SearchRequest("test_index01");
  
     // 创建一个查询
     QueryBuilder queryBuilder = QueryBuilders.matchAllQuery();
     // 创建一个搜索
     SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();
  
     sourceBuilder.query(queryBuilder);
     sourceBuilder.sort("age");
     // sourceBuilder.sort("age", SortOrder.ASC); (默认！)
     // sourceBuilder.sort("age", SortOrder.DESC);
     sourceBuilder.from(0);
     sourceBuilder.size(3);
  
     request.source(sourceBuilder);
     SearchResponse response = client.search(request, RequestOptions.DEFAULT);
     SearchHits hits = response.getHits();
     
     for (SearchHit hit : hits) {
        System.out.println(hit);
     }
  }
  ```

  还是和请求体一致：使用SearchSourceBuilder的方法：

  `sort()`:选择字段排序，默认是升序排列

  `from()`、`size()`:分页的相关设置，起始位置和页面大小
  结果：

  ```json
  {
    "_index" : "test_index01",
    "_type" : "_doc",
    "_id" : "4",
    "_score" : null,
    "_source" : {
      "age" : 16,
      "name" : "EEEE"
    },
    "sort" : [
      16
    ]
  }
  {
    "_index" : "test_index01",
    "_type" : "_doc",
    "_id" : "2",
    "_score" : null,
    "_source" : {
      "age" : 18,
      "name" : "CCCC"
    },
    "sort" : [
      18
    ]
  }
  {
    "_index" : "test_index01",
    "_type" : "_doc",
    "_id" : "0",
    "_score" : null,
    "_source" : {
      "age" : 19,
      "name" : "AAAA"
    },
    "sort" : [
      19
    ]
  }
  ```

   

  > 精准查询term

  ```java
  @Test
  void testTerm() throws IOException {
     SearchRequest request = new SearchRequest("test_index01");
     SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();
     QueryBuilder queryBuilder = QueryBuilders.termQuery("name", "aaaa");
     //QueryBuilder queryBuilder = QueryBuilders
     //                             .termsQuery("name", "bbbb", "cccc", "dddd");
     sourceBuilder.query(queryBuilder);
     request.source(sourceBuilder);
     SearchResponse response = client.search(request, RequestOptions.DEFAULT);
     SearchHits hits = response.getHits();
     for (SearchHit hit : hits) {
        System.out.println(hit);
     }
  }
  ```

  ==注意！！==：不知是Bug还是默认的规则,当我们的==字段值包含了大写英文字母，在经过分词解析的时候会变为小写！(除了keyword外)==但是显示的数据仍然是大写。

  上面的实例中如果是term(“name”,”AAAA”)是匹配不到{“name”:”AAAA”,”age”:19}这条记录的。因为在分词解析过程中变为了小写：

  ![image-20200527223117837](https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200527223117.png)

   这一点在match中不必多虑，因为搜索词也会被同样处理😄。

  

  > 多条件匹配(bool-query之must、notmust、should、filter)

  ```java
  @Test
  	void testBoolQuery() throws IOException {
  		SearchRequest request = new SearchRequest("test_index01");
  		SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();
  		BoolQueryBuilder queryBuilder = QueryBuilders.boolQuery()
  				.must(QueryBuilders.matchQuery("name", "EEEE"))
  				.filter(QueryBuilders.rangeQuery("age").gte(15).lte(24))
                 .mustNot(QueryBuilders.rangeQuery("id").gte(5));
  		sourceBuilder.query(queryBuilder);
  		request.source(sourceBuilder);
  		SearchResponse response = client.search(request, RequestOptions.DEFAULT);
  		SearchHits hits = response.getHits();
  		hits.forEach(System.out::println);
  	}
  ```

  选择了代表性较高的must、mustNot和rangeQuery

   

  > 多值查询

  - 使用terms，在term中有演示
  - match中匹配值以空格隔开

  ```java
  @Test
  void testMatch() throws IOException {
     SearchRequest request = new SearchRequest("test_index01");
     SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();
     QueryBuilder queryBuilder = QueryBuilders.matchQuery("name", "BBBB CCCC EEEE");
     sourceBuilder.query(queryBuilder);
     sourceBuilder.fetchSource("name", null);
  
     request.source(sourceBuilder);
     SearchResponse response = client.search(request, RequestOptions.DEFAULT);
     SearchHits hits = response.getHits();
     for (SearchHit hit : hits) {
        System.out.println(hit);
     }
  }
  ```

  匹配值是“BBBB CCCC EEEE”，可以拆分为“bbbb”、“cccc”、“eeee”

  ![image-20200527225126755](https://picbed-sakura.oss-cn-shanghai.aliyuncs.com/notePic/20200527225126.png)

   

  > 高亮查询！

  ```java
  @Test
  void testHighlight() throws IOException {
     SearchRequest request = new SearchRequest("test_index01");
     SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();
     QueryBuilder queryBuilder = QueryBuilders.matchQuery("name", "CCCC");
     HighlightBuilder highlightBuilder = new HighlightBuilder();
     highlightBuilder.field("name");
     highlightBuilder.preTags("<span class='key' style='color:red'>");
     highlightBuilder.postTags("</span>");
  
     sourceBuilder.highlighter(highlightBuilder);
     sourceBuilder.query(queryBuilder);
     request.source(sourceBuilder);
     SearchResponse response = client.search(request, RequestOptions.DEFAULT);
     SearchHits hits = response.getHits();
     hits.forEach(System.out::println);
  }
  ```

  出现了一个新的类：`HighlightBuilder`，其他使用和命令行的操作差不多。一样设置pre_tags和post_tags
  结果：

  ```json
  {
    "_index" : "test_index01",
    "_type" : "_doc",
    "_id" : "2",
    "_score" : 1.6739764,
    "_source" : {
      "age" : 18,
      "name" : "CCCC"
    },
    "highlight" : {
      "name" : [
        "<span class='key' style='color:red'>CCCC</span>"
      ]
    }
  }
  ```





## 五、实战

### Java爬虫基础

`jsoup`(本次使用)、`tika`

```xml
<dependency>
    <groupId>org.jsoup</groupId>
    <artifactId>jsoup</artifactId>
    <version>1.10.2</version>
</dependency>
```